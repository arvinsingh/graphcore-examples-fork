{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dbd51d48",
   "metadata": {},
   "source": [
    "# Magma Inference Demo\n",
    "\n",
    "Copyright (c) 2023 Graphcore Ltd."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ceb61",
   "metadata": {},
   "source": [
    "This notebook provides a basic interactive MAGMA inference application, based on the freely available checkpoint delivered by Aleph Alpha. Note that such checkpoint is only a demo meant to help users understand how the model works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5855cc08",
   "metadata": {},
   "source": [
    "To run this notebook, make sure you have configured the environment as explained in the repository README."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b0eee0",
   "metadata": {},
   "source": [
    "The main inputs are the `image url` and the `text` prompt.\n",
    "The image can be chosen between the provided examples or you can use an image url.\n",
    "The text prompt can be the one that you prefer. In this interactive application the maximum allowed sequence length is by default 500. To use longer text prompts, you can change the key used to initialise the session from  `magma_v1_500` to `magma_v1_1024`.\n",
    "Note that the output is sensitive to small changes to the prompt.\n",
    "\n",
    "This interactive application allows you to control generation playing around with the `temperature`, `top-p` and `top-k` parameters. Moreover, the random `seed` is always explicitly set in order to have reproducible results even in presence of randomness.\n",
    "\n",
    "- **top-p**: probability is redistributed among the first x tokens such that the cumulative probability is greater than the specified threshold p. Then, next token is sampled from such distribution (categorical sampling, non deterministic).\n",
    "- **top-k**: probability is redistributed among the K most-likely tokens. Then, next token is sampled from such distribution (categorical sampling, non deterministic).\n",
    "- **temperature**: logits are scaled by a factor 1/T (T between 0 and 1 ) before applying the softmax. This makes the distribution more peaked for low temperature, and broader for high temperatures. A zero temperature corresponds to a deterministic choice (argmax), while sampling output becomes more random as we increase the temperature.\n",
    "\n",
    "If you are not familiar with these concepts, this [great hugging face article](https://huggingface.co/blog/how-to-generate) can help you visualise them.\n",
    "\n",
    "You can also change the number of generated tokens by varying the `max_out_tokens` parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a01e032",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/sofial/workspace/venvs/poplar_sdk-ubuntu_20_04-3.2.0-EA.1+1213-ec6c27ac64/3.2.0-EA.1+1213_popart/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libc10_hip.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from run_inference import run_inference, init_inference_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a6569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nethome/sofial/workspace/sdks/poplar_sdk-ubuntu_20_04-3.2.0-EA.1+1213-ec6c27ac64/poplar-ubuntu_20_04-3.2.0+6970-37744fc347\n"
     ]
    }
   ],
   "source": [
    "import sys, os, os.path\n",
    "\n",
    "os.environ[\"POPART_LOG_LEVEL\"] = \"ERROR\"\n",
    "print(os.environ[\"POPLAR_SDK_ENABLED\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04412adc",
   "metadata": {},
   "source": [
    "## Compile and load the model\n",
    "Compilation takes around 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c82287b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-10 14:05:19 INFO: Starting. Process id: 60859\n",
      "2023-01-10 14:05:19 INFO: Config: MagmaConfig(seed=0, visual=ResNetConfig(layers=(6, 8, 18, 8), width=96, image_resolution=384, execution=ResnetExecution(micro_batch_size=1, available_memory_proportion=(1.0, 1.0, 1.0, 1.0)), precision=<Precision.float16: popxl.dtypes.float16>), transformer=GPTJConfig(layers=28, hidden_size=4096, sequence_length=500, precision=<Precision.float16: popxl.dtypes.float16>, embedding=GPTJConfig.Embedding(vocab_size=50400, real_vocab_size=50258), attention=GPTJConfig.Attention(heads=16, rotary_positional_embeddings_base=10000, rotary_dim=64, use_cache=False), execution=GPTJExecution(micro_batch_size=1, available_memory_proportion=(0.45,), tensor_parallel=4, attention_serialisation=1), att_adapter=GPTJConfig.Adapter(layer_norm=False, downsample_factor=8, mode=None), ff_adapter=GPTJConfig.Adapter(layer_norm=False, downsample_factor=4, mode='normal')))\n",
      "2023-01-10 14:05:19 INFO: Starting PopXL IR construction\n",
      "2023-01-10 14:06:39 INFO: PopXL IR construction duration: 1.32 mins\n",
      "2023-01-10 14:06:39 INFO: Starting PopXL compilation\n",
      "2023-01-10 14:09:46 INFO: PopXL compilation duration: 3.12 mins\n",
      "2023-01-10 14:09:46 INFO: Starting Loading magma weights to host\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: The compile time engine option debug.branchRecordTile is set to \"5887\" when creating the Engine. (At compile-tile it was set to 1471)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GPTJ language model...\n",
      "loading magma checkpoint from: ./mp_rank_00_model_states.pt\n",
      "magma successfully loaded\n",
      "2023-01-10 14:11:16 INFO: Loading magma weights to host duration: 1.50 mins\n",
      "2023-01-10 14:11:16 INFO: Starting Loading magma pretrained model to IPU\n",
      "2023-01-10 14:14:51 INFO: Loading magma pretrained model to IPU duration: 3.58 mins\n"
     ]
    }
   ],
   "source": [
    "session, config, tokenizer = init_inference_session(\n",
    "    \"magma_v1_500\"\n",
    ")  #  magma_v1_500 magma_v1_1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478b8429",
   "metadata": {},
   "source": [
    "## Run demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74bb21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import ipywidgets as ipw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f2a70d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1aa570d21604aea9a3ccc54b102ff15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='image_url', layout=Layout(width='600px'), options=('https://www.arâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def answer_int(image_url, text, seed, top_p, top_k, temperature, max_out_tokens):\n",
    "\n",
    "    if image_url.startswith(\"http\"):\n",
    "        response = requests.get(image_url)\n",
    "        image = BytesIO(response.content)\n",
    "    else:\n",
    "        image = open(image_url, \"rb\")\n",
    "    img = ipw.Image(value=image.read(), width=384, height=384)\n",
    "    prompt = ipw.Label(value=f\"Prompt: {text}\", style={\"font_size\": \"16px\"})\n",
    "    answer = ipw.Label(\n",
    "        value=f\"Answer: `{run_inference(session, config, tokenizer, image_url, text, seed, top_p, top_k, temperature, max_out_tokens)}`\",\n",
    "        style={\"font_size\": \"16px\"},\n",
    "    )\n",
    "    return ipw.VBox(\n",
    "        [img, prompt, answer], layout=ipw.Layout(display=\"flex\", align_items=\"center\")\n",
    "    )\n",
    "\n",
    "\n",
    "il = ipw.Layout(width=\"600px\")\n",
    "\n",
    "image_choices = [\n",
    "    \"https://www.art-prints-on-demand.com/kunst/thomas_cole/woods_hi.jpg\",\n",
    "    \"demo_example_images/cantaloupe_popsicle.jpg\",\n",
    "    \"demo_example_images/circles.jpg\",\n",
    "    \"demo_example_images/circles_square.jpg\",\n",
    "    \"demo_example_images/korea.jpg\",\n",
    "    \"demo_example_images/matterhorn.jpg\",\n",
    "    \"demo_example_images/mushroom.jpg\",\n",
    "    \"demo_example_images/people.jpg\",\n",
    "    \"demo_example_images/playarea.jpg\",\n",
    "    \"demo_example_images/popsicle.png\",\n",
    "    \"demo_example_images/rainbow_popsicle.jpeg\",\n",
    "    \"demo_example_images/table_tennis.jpg\",\n",
    "]\n",
    "ipw.interact(\n",
    "    answer_int,\n",
    "    image_url=ipw.Dropdown(\n",
    "        options=image_choices,\n",
    "        value=\"https://www.art-prints-on-demand.com/kunst/thomas_cole/woods_hi.jpg\",\n",
    "        layout=il,\n",
    "        continuous_update=False,\n",
    "    ),\n",
    "    text=ipw.Text(value=\"A painting of \", layout=il, continuous_update=False),\n",
    "    seed=ipw.IntSlider(0, 0, 300, layout=il, continuous_update=False),\n",
    "    top_p=(\n",
    "        ipw.FloatSlider(\n",
    "            value=0.9, min=0.0, max=1.0, step=0.01, layout=il, continuous_update=False\n",
    "        )\n",
    "    ),\n",
    "    top_k=ipw.IntSlider(0, 0, 10, layout=il, continuous_update=False),\n",
    "    temperature=ipw.FloatSlider(\n",
    "        value=0.7, min=0.0, max=1.0, step=0.01, layout=il, continuous_update=False\n",
    "    ),\n",
    "    max_out_tokens=ipw.IntSlider(6, 1, 356, layout=il, continuous_update=False),\n",
    ");"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
